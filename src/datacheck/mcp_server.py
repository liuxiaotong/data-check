"""DataCheck MCP Server - Model Context Protocol æœåŠ¡."""

import math
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List

try:
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import Tool, TextContent

    HAS_MCP = True
except ImportError:
    HAS_MCP = False

from datacheck.checker import DataChecker
from datacheck.fixer import DataFixer
from datacheck.report import QualityReport
from datacheck.rules import RuleSet, get_sft_ruleset, get_preference_ruleset


def create_server() -> "Server":
    """åˆ›å»º MCP æœåŠ¡å™¨å®ä¾‹."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = Server("datacheck")

    @server.list_tools()
    async def list_tools() -> List[Tool]:
        """åˆ—å‡ºå¯ç”¨çš„å·¥å…·."""
        return [
            Tool(
                name="check_data_quality",
                description="æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„è´¨é‡ (æ”¯æŒ JSON/JSONL/CSV)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "schema_path": {
                            "type": "string",
                            "description": "Schema æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                        "ruleset": {
                            "type": "string",
                            "enum": ["default", "sft", "preference"],
                            "description": "è§„åˆ™é›†ï¼ˆé»˜è®¤: defaultï¼‰",
                        },
                        "sample_count": {
                            "type": "integer",
                            "description": "éšæœºæŠ½æ ·æ•°é‡ï¼ˆå¯é€‰ï¼‰",
                        },
                        "sample_rate": {
                            "type": "number",
                            "description": "éšæœºæŠ½æ ·æ¯”ä¾‹ 0-1ï¼ˆå¯é€‰ï¼‰",
                        },
                    },
                    "required": ["data_path"],
                },
            ),
            Tool(
                name="validate_from_datarecipe",
                description="ä½¿ç”¨ DataRecipe åˆ†æç»“æœéªŒè¯æ•°æ®",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "analysis_dir": {
                            "type": "string",
                            "description": "DataRecipe åˆ†æè¾“å‡ºç›®å½•",
                        },
                        "data_path": {
                            "type": "string",
                            "description": "è¦éªŒè¯çš„æ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤éªŒè¯åˆæˆæ•°æ®ï¼‰",
                        },
                    },
                    "required": ["analysis_dir"],
                },
            ),
            Tool(
                name="compare_distributions",
                description="å¯¹æ¯”å¤šä¸ªæ•°æ®æ–‡ä»¶çš„åˆ†å¸ƒ (æ”¯æŒ JSON/JSONL/CSV)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "file_paths": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "è¦å¯¹æ¯”çš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (JSON/JSONL/CSV)",
                        },
                    },
                    "required": ["file_paths"],
                },
            ),
            Tool(
                name="list_quality_rules",
                description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è´¨é‡æ£€æŸ¥è§„åˆ™",
                inputSchema={
                    "type": "object",
                    "properties": {},
                },
            ),
            Tool(
                name="infer_schema",
                description="ä»æ•°æ®æ–‡ä»¶æ¨æ–­ Schema (å­—æ®µç±»å‹ã€çº¦æŸã€å¿…å¡«é¡¹)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "output_path": {
                            "type": "string",
                            "description": "Schema è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                    },
                    "required": ["data_path"],
                },
            ),
            Tool(
                name="fix_data",
                description="ä¿®å¤æ•°æ®æ–‡ä»¶å¸¸è§è´¨é‡é—®é¢˜ (å»é‡ã€å»ç©ºç™½ã€PII è„±æ•)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "output_path": {
                            "type": "string",
                            "description": "ä¿®å¤åæ–‡ä»¶è¾“å‡ºè·¯å¾„ (JSONL)",
                        },
                        "strip_pii": {
                            "type": "boolean",
                            "description": "æ˜¯å¦è„±æ• PII ä¿¡æ¯ï¼ˆé»˜è®¤: falseï¼‰",
                        },
                    },
                    "required": ["data_path", "output_path"],
                },
            ),
            Tool(
                name="batch_check_directory",
                description="æ‰¹é‡æ£€æŸ¥ç›®å½•ä¸‹æ‰€æœ‰æ•°æ®æ–‡ä»¶çš„è´¨é‡ (é€’å½’æ‰«æ JSON/JSONL/CSV)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "directory": {
                            "type": "string",
                            "description": "è¦æ£€æŸ¥çš„ç›®å½•è·¯å¾„",
                        },
                        "schema_path": {
                            "type": "string",
                            "description": "Schema æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                        "ruleset": {
                            "type": "string",
                            "enum": ["default", "sft", "preference"],
                            "description": "è§„åˆ™é›†ï¼ˆé»˜è®¤: defaultï¼‰",
                        },
                        "pattern": {
                            "type": "string",
                            "description": "æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤: *.json,*.jsonl,*.csvï¼‰",
                        },
                        "sample_count": {
                            "type": "integer",
                            "description": "æ¯ä¸ªæ–‡ä»¶çš„éšæœºæŠ½æ ·æ•°é‡ï¼ˆå¯é€‰ï¼‰",
                        },
                    },
                    "required": ["directory"],
                },
            ),
            Tool(
                name="check_drift",
                description="æ£€æµ‹ä¸¤ä¸ªæ•°æ®æ–‡ä»¶ä¹‹é—´çš„åˆ†å¸ƒæ¼‚ç§»ï¼ˆæ•°å€¼ç»Ÿè®¡å·®å¼‚ã€ç±»åˆ«åˆ†å¸ƒå˜åŒ–ã€æ–‡æœ¬ç‰¹å¾å¯¹æ¯”ï¼‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path_a": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶ A è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "data_path_b": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶ B è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "fields": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "è¦å¯¹æ¯”çš„å­—æ®µåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤å¯¹æ¯”æ‰€æœ‰å…±æœ‰å­—æ®µï¼‰",
                        },
                    },
                    "required": ["data_path_a", "data_path_b"],
                },
            ),
            Tool(
                name="check_leakage",
                description="æ£€æµ‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ•°æ®æ³„æ¼ï¼ˆå®Œå…¨é‡å¤ + token Jaccard è¿‘ä¼¼é‡å¤ï¼‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "train_path": {
                            "type": "string",
                            "description": "è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "test_path": {
                            "type": "string",
                            "description": "æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "key_fields": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "ç”¨äºæ¯”è¾ƒçš„å­—æ®µï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡æœ¬å­—æ®µï¼‰",
                        },
                        "threshold": {
                            "type": "number",
                            "description": "è¿‘ä¼¼é‡å¤çš„ Jaccard ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.9ï¼‰",
                            "default": 0.9,
                        },
                    },
                    "required": ["train_path", "test_path"],
                },
            ),
            Tool(
                name="check_bias",
                description="æ£€æµ‹æ•°æ®é›†åå·®ï¼ˆç±»åˆ«ä¸å‡è¡¡ã€æ–‡æœ¬é•¿åº¦åˆ†å¸ƒåå·®ã€è¯­è¨€åˆ†å¸ƒåå·®ï¼‰",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "label_field": {
                            "type": "string",
                            "description": "æ ‡ç­¾å­—æ®µåï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰",
                        },
                        "text_field": {
                            "type": "string",
                            "description": "æ–‡æœ¬å­—æ®µåï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰",
                        },
                        "dimensions": {
                            "type": "array",
                            "items": {
                                "type": "string",
                                "enum": ["category", "length", "language", "all"],
                            },
                            "description": "æ£€æµ‹ç»´åº¦ï¼ˆé»˜è®¤ allï¼‰",
                            "default": ["all"],
                        },
                    },
                    "required": ["data_path"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
        """è°ƒç”¨å·¥å…·."""

        if name == "check_data_quality":
            ruleset_name = arguments.get("ruleset", "default")
            if ruleset_name == "sft":
                rules = get_sft_ruleset()
            elif ruleset_name == "preference":
                rules = get_preference_ruleset()
            else:
                rules = RuleSet()

            checker = DataChecker(rules)
            result = checker.check_file(
                arguments["data_path"],
                arguments.get("schema_path"),
                sample_count=arguments.get("sample_count"),
                sample_rate=arguments.get("sample_rate"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"æ£€æŸ¥å¤±è´¥: {result.error}")]

            # Generate summary
            score = result.pass_rate * 100
            if score >= 90:
                grade = "ğŸŸ¢ ä¼˜ç§€"
            elif score >= 70:
                grade = "ğŸŸ¡ è‰¯å¥½"
            elif score >= 50:
                grade = "ğŸŸ  ä¸€èˆ¬"
            else:
                grade = "ğŸ”´ éœ€æ”¹è¿›"

            summary = f"""## æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ

### æ¦‚è¦
- æ€»æ ·æœ¬: {result.total_samples}
- é€šè¿‡: {result.passed_samples}
- å¤±è´¥: {result.failed_samples}
- **é€šè¿‡ç‡: {result.pass_rate:.1%}**
- **è¯„çº§: {grade}**

### é—®é¢˜ç»Ÿè®¡
- ğŸ”´ é”™è¯¯: {result.error_count}
- ğŸŸ¡ è­¦å‘Š: {result.warning_count}
- ğŸ”µ æç¤º: {result.info_count}
"""

            if result.duplicates:
                summary += f"\n### é‡å¤æ£€æµ‹\nå‘ç° {len(result.duplicates)} ç»„é‡å¤æ•°æ®\n"

            if result.failed_sample_ids:
                summary += f"\n### å¤±è´¥æ ·æœ¬\n{', '.join(result.failed_sample_ids[:10])}"
                if len(result.failed_sample_ids) > 10:
                    summary += f" (è¿˜æœ‰ {len(result.failed_sample_ids) - 10} ä¸ª...)"

            return [TextContent(type="text", text=summary)]

        elif name == "validate_from_datarecipe":
            checker = DataChecker()
            result = checker.check_from_datarecipe(
                arguments["analysis_dir"],
                arguments.get("data_path"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"éªŒè¯å¤±è´¥: {result.error}")]

            report = QualityReport(result, title="æ•°æ®éªŒè¯æŠ¥å‘Š")

            # Save report
            output_dir = Path(arguments["analysis_dir"]) / "12_è´¨æ£€æŠ¥å‘Š"
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / "quality_report.md"
            report.save(str(output_path), "markdown")

            # Return summary
            score = result.pass_rate * 100
            grade = (
                "ğŸŸ¢ ä¼˜ç§€"
                if score >= 90
                else "ğŸŸ¡ è‰¯å¥½"
                if score >= 70
                else "ğŸŸ  ä¸€èˆ¬"
                if score >= 50
                else "ğŸ”´ éœ€æ”¹è¿›"
            )

            return [
                TextContent(
                    type="text",
                    text=f"""## æ•°æ®éªŒè¯å®Œæˆ

### ç»“æœ
- é€šè¿‡ç‡: **{result.pass_rate:.1%}**
- è¯„çº§: **{grade}**
- æ€»æ ·æœ¬: {result.total_samples}
- é”™è¯¯: {result.error_count}, è­¦å‘Š: {result.warning_count}

### æŠ¥å‘Š
å·²ä¿å­˜åˆ°: {output_path}

{"### é‡å¤æ•°æ®" + chr(10) + f"å‘ç° {len(result.duplicates)} ç»„é‡å¤" if result.duplicates else ""}
""",
                )
            ]

        elif name == "compare_distributions":
            file_paths = arguments["file_paths"]

            if len(file_paths) < 2:
                return [TextContent(type="text", text="é”™è¯¯: è‡³å°‘éœ€è¦ 2 ä¸ªæ–‡ä»¶")]

            distributions = []
            for file_path in file_paths:
                checker = DataChecker()
                samples, _ = checker._load_data(Path(file_path))
                result = checker.check(samples, {})

                distributions.append(
                    {
                        "file": Path(file_path).name,
                        "count": len(samples),
                        "dist": result.distribution,
                    }
                )

            # Build comparison
            lines = ["## æ•°æ®åˆ†å¸ƒå¯¹æ¯”", ""]
            lines.append("| æ–‡ä»¶ | æ ·æœ¬æ•° |")
            lines.append("|------|--------|")
            for d in distributions:
                lines.append(f"| {d['file']} | {d['count']} |")

            lines.extend(["", "### å­—æ®µç»Ÿè®¡", ""])

            all_fields = set()
            for d in distributions:
                all_fields.update(d["dist"].get("fields", {}).keys())

            for field in sorted(all_fields):
                lines.append(f"**{field}**:")
                for d in distributions:
                    field_data = d["dist"].get("fields", {}).get(field, {})
                    if "length_stats" in field_data:
                        stats = field_data["length_stats"]
                        lines.append(f"- {d['file']}: é•¿åº¦ {stats['avg']:.0f} (avg)")

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "list_quality_rules":
            ruleset = RuleSet()
            lines = ["## å¯ç”¨è´¨é‡æ£€æŸ¥è§„åˆ™", ""]

            for rule in ruleset.rules.values():
                severity_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(
                    rule.severity.value, "âšª"
                )
                status = "âœ“" if rule.enabled else "âœ—"
                lines.append(f"- {status} **{rule.name}** {severity_icon}")
                lines.append(f"  - ID: `{rule.id}`")
                lines.append(f"  - {rule.description}")
                lines.append("")

            lines.extend(
                [
                    "## é¢„è®¾è§„åˆ™é›†",
                    "- `default`: é€šç”¨è§„åˆ™",
                    "- `sft`: SFT æ•°æ®è§„åˆ™",
                    "- `preference`: åå¥½æ•°æ®è§„åˆ™",
                ]
            )

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "infer_schema":
            checker = DataChecker()
            schema = checker.infer_schema_file(
                arguments["data_path"],
                arguments.get("output_path"),
            )

            fields = schema.get("fields", {})
            field_count = len(fields)
            required_count = sum(1 for f in fields.values() if f.get("required"))

            lines = [
                "## Schema æ¨æ–­ç»“æœ",
                "",
                f"- æ ·æœ¬æ•°: {schema.get('sample_count', 0)}",
                f"- å­—æ®µæ•°: {field_count}",
                f"- å¿…å¡«å­—æ®µ: {required_count}",
                "",
                "### å­—æ®µè¯¦æƒ…",
                "",
                "| å­—æ®µ | ç±»å‹ | å¿…å¡« | çº¦æŸ |",
                "|------|------|------|------|",
            ]

            for fname, fdef in fields.items():
                ftype = fdef.get("type", "-")
                req = "æ˜¯" if fdef.get("required") else "å¦"
                constraints = []
                if "min_length" in fdef:
                    constraints.append(f"é•¿åº¦ {fdef['min_length']}-{fdef['max_length']}")
                if "enum" in fdef:
                    constraints.append(f"æšä¸¾ {fdef['enum']}")
                if "min_value" in fdef:
                    constraints.append(f"å€¼ {fdef['min_value']}-{fdef['max_value']}")
                lines.append(f"| {fname} | {ftype} | {req} | {', '.join(constraints) or '-'} |")

            if arguments.get("output_path"):
                lines.extend(["", f"Schema å·²ä¿å­˜: {arguments['output_path']}"])

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "fix_data":
            fixer = DataFixer()
            result = fixer.fix_file(
                arguments["data_path"],
                arguments["output_path"],
                strip_pii=arguments.get("strip_pii", False),
            )

            lines = [
                "## æ•°æ®ä¿®å¤ç»“æœ",
                "",
                f"- è¾“å…¥æ ·æœ¬: {result.total_input}",
                f"- è¾“å‡ºæ ·æœ¬: {result.total_output}",
            ]
            if result.duplicates_removed:
                lines.append(f"- å»é™¤é‡å¤: {result.duplicates_removed}")
            if result.trimmed_count:
                lines.append(f"- ä¿®å‰ªç©ºç™½: {result.trimmed_count} ä¸ªå­—æ®µ")
            if result.empty_removed:
                lines.append(f"- ç§»é™¤ç©ºæ ·æœ¬: {result.empty_removed}")
            if result.pii_redacted_count:
                lines.append(f"- PII è„±æ•: {result.pii_redacted_count} ä¸ªå­—æ®µ")
            lines.extend(["", f"è¾“å‡ºæ–‡ä»¶: {arguments['output_path']}"])

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "batch_check_directory":
            ruleset_name = arguments.get("ruleset", "default")
            if ruleset_name == "sft":
                rules = get_sft_ruleset()
            elif ruleset_name == "preference":
                rules = get_preference_ruleset()
            else:
                rules = RuleSet()

            checker = DataChecker(rules)
            pat = arguments.get("pattern")
            patterns = [p.strip() for p in pat.split(",")] if pat else None

            batch_result = checker.check_directory(
                arguments["directory"],
                schema_path=arguments.get("schema_path"),
                patterns=patterns,
                sample_count=arguments.get("sample_count"),
            )

            lines = [
                "## æ‰¹é‡æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ",
                "",
                f"- ç›®å½•: `{batch_result.directory}`",
                f"- æ–‡ä»¶æ•°: {batch_result.total_files}",
                f"- æ€»æ ·æœ¬: {batch_result.total_samples}",
                f"- **æ€»é€šè¿‡ç‡: {batch_result.overall_pass_rate:.1%}**",
                "",
                "### æ–‡ä»¶æ˜ç»†",
                "",
                "| æ–‡ä»¶ | æ ·æœ¬æ•° | é€šè¿‡ç‡ | é”™è¯¯ | è­¦å‘Š |",
                "|------|--------|--------|------|------|",
            ]

            for fp, fr in batch_result.file_results.items():
                lines.append(
                    f"| {fp} | {fr.total_samples} | {fr.pass_rate:.1%} "
                    f"| {fr.error_count} | {fr.warning_count} |"
                )

            if batch_result.skipped_files:
                lines.extend(["", f"### è·³è¿‡æ–‡ä»¶ ({len(batch_result.skipped_files)})"])
                for s in batch_result.skipped_files:
                    lines.append(f"- {s}")

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "check_drift":
            checker = DataChecker()
            samples_a, _ = checker._load_data(Path(arguments["data_path_a"]))
            samples_b, _ = checker._load_data(Path(arguments["data_path_b"]))
            if not samples_a or not samples_b:
                return [TextContent(type="text", text="é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸ºç©º")]

            fields_a = set(samples_a[0].keys())
            fields_b = set(samples_b[0].keys())
            shared = sorted(fields_a & fields_b)
            requested = arguments.get("fields")
            if requested:
                shared = [f for f in requested if f in shared]
            if not shared:
                return [TextContent(type="text", text="é”™è¯¯: ä¸¤ä¸ªæ–‡ä»¶æ²¡æœ‰å…±æœ‰å­—æ®µ")]

            def _classify(vals: list) -> str:
                nums = [v for v in vals if isinstance(v, (int, float))]
                if len(nums) > len(vals) * 0.5:
                    return "numeric"
                strs = [v for v in vals if isinstance(v, str)]
                if strs:
                    unique_ratio = len(set(strs)) / len(strs) if strs else 1
                    if unique_ratio < 0.3:
                        return "categorical"
                    return "text"
                return "text"

            lines = [
                "## åˆ†å¸ƒæ¼‚ç§»æ£€æµ‹ç»“æœ", "",
                f"- æ–‡ä»¶ A: `{Path(arguments['data_path_a']).name}` ({len(samples_a)} æ¡)",
                f"- æ–‡ä»¶ B: `{Path(arguments['data_path_b']).name}` ({len(samples_b)} æ¡)",
                f"- å…±æœ‰å­—æ®µ: {len(shared)}", "",
            ]
            for field in shared:
                vals_a = [s.get(field) for s in samples_a if field in s]
                vals_b = [s.get(field) for s in samples_b if field in s]
                ftype = _classify(vals_a + vals_b)
                lines.append(f"### å­—æ®µ: `{field}` (ç±»å‹: {ftype})")
                lines.append("")
                if ftype == "numeric":
                    for label, vals in [("A", vals_a), ("B", vals_b)]:
                        nums = [v for v in vals if isinstance(v, (int, float))]
                        if nums:
                            avg = sum(nums) / len(nums)
                            lines.append(f"- {label}: count={len(nums)}, mean={avg:.2f}, min={min(nums)}, max={max(nums)}")
                elif ftype == "categorical":
                    dist_a = Counter(str(v) for v in vals_a)
                    dist_b = Counter(str(v) for v in vals_b)
                    all_cats = sorted(set(list(dist_a.keys()) + list(dist_b.keys())))
                    lines.append("| ç±»åˆ« | A | B |")
                    lines.append("|------|---|---|")
                    for cat in all_cats[:20]:
                        lines.append(f"| {cat} | {dist_a.get(cat, 0)} | {dist_b.get(cat, 0)} |")
                else:
                    for label, vals in [("A", vals_a), ("B", vals_b)]:
                        lengths = [len(str(v)) for v in vals if v]
                        if lengths:
                            avg_len = sum(lengths) / len(lengths)
                            lines.append(f"- {label}: count={len(lengths)}, avg_len={avg_len:.0f}")
                lines.append("")
            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "check_leakage":
            checker = DataChecker()
            train_samples, _ = checker._load_data(Path(arguments["train_path"]))
            test_samples, _ = checker._load_data(Path(arguments["test_path"]))
            if not train_samples or not test_samples:
                return [TextContent(type="text", text="é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸ºç©º")]

            threshold = arguments.get("threshold", 0.9)
            key_fields = arguments.get("key_fields")
            if not key_fields:
                sample = train_samples[0]
                key_fields = [k for k, v in sample.items() if isinstance(v, str) and len(v) > 10]
                if not key_fields:
                    key_fields = [k for k, v in sample.items() if isinstance(v, str)]
            if not key_fields:
                return [TextContent(type="text", text="é”™è¯¯: æœªæ‰¾åˆ°å¯æ¯”è¾ƒçš„æ–‡æœ¬å­—æ®µ")]

            def _make_key(s: dict, fields: list) -> str:
                return "|||".join(str(s.get(f, "")).strip() for f in fields)

            train_keys: Dict[str, int] = {}
            for i, s in enumerate(train_samples):
                k = _make_key(s, key_fields)
                if k not in train_keys:
                    train_keys[k] = i

            exact_dupes = []
            for j, s in enumerate(test_samples):
                k = _make_key(s, key_fields)
                if k in train_keys:
                    exact_dupes.append((train_keys[k], j))

            # Near-duplicate via Jaccard
            def _tokenize(text: str) -> set:
                return set(text.lower().split())

            train_tokens = []
            for s in train_samples[:5000]:
                combined = " ".join(str(s.get(f, "")) for f in key_fields)
                train_tokens.append(_tokenize(combined))

            exact_test_idx = set(j for _, j in exact_dupes)
            near_dupes = []
            for j in range(min(len(test_samples), 500)):
                if j in exact_test_idx:
                    continue
                test_tok = _tokenize(" ".join(str(test_samples[j].get(f, "")) for f in key_fields))
                if not test_tok:
                    continue
                for i in range(len(train_tokens)):
                    inter = len(test_tok & train_tokens[i])
                    union = len(test_tok | train_tokens[i])
                    sim = inter / union if union else 0
                    if threshold <= sim < 1.0:
                        near_dupes.append((i, j, sim))
                        break

            total = len(test_samples)
            exact_rate = len(exact_dupes) / total * 100 if total else 0
            near_rate = len(near_dupes) / total * 100 if total else 0
            total_rate = (len(exact_dupes) + len(near_dupes)) / total * 100 if total else 0

            lines = [
                "## æ•°æ®æ³„æ¼æ£€æµ‹ç»“æœ", "",
                f"- è®­ç»ƒé›†: {len(train_samples)} æ¡",
                f"- æµ‹è¯•é›†: {len(test_samples)} æ¡",
                f"- æ¯”è¾ƒå­—æ®µ: {', '.join(key_fields)}",
                f"- å®Œå…¨é‡å¤: {len(exact_dupes)} æ¡ ({exact_rate:.2f}%)",
                f"- è¿‘ä¼¼é‡å¤: {len(near_dupes)} æ¡ ({near_rate:.2f}%)",
                f"- **æ€»æ³„æ¼: {len(exact_dupes) + len(near_dupes)} æ¡ ({total_rate:.2f}%)**",
            ]
            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "check_bias":
            checker = DataChecker()
            samples, _ = checker._load_data(Path(arguments["data_path"]))
            if not samples:
                return [TextContent(type="text", text="é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸ºç©º")]

            label_field = arguments.get("label_field")
            text_field = arguments.get("text_field")
            dimensions = arguments.get("dimensions", ["all"])
            if "all" in dimensions:
                dimensions = ["category", "length", "language"]

            if not label_field or not text_field:
                sample = samples[0]
                for k, v in sample.items():
                    if not label_field and isinstance(v, str) and len(v) < 50:
                        unique_vals = set(s.get(k, "") for s in samples[:200])
                        if 2 <= len(unique_vals) <= 50:
                            label_field = k
                    if not text_field and isinstance(v, str) and len(v) >= 50:
                        text_field = k

            lines = [
                "## æ•°æ®åå·®æ£€æµ‹ç»“æœ", "",
                f"- æ–‡ä»¶: `{Path(arguments['data_path']).name}` ({len(samples)} æ¡)",
                f"- æ ‡ç­¾å­—æ®µ: `{label_field or '(æœªæŒ‡å®š)'}`",
                f"- æ–‡æœ¬å­—æ®µ: `{text_field or '(æœªæŒ‡å®š)'}`", "",
            ]

            if "category" in dimensions and label_field:
                labels = [s.get(label_field) for s in samples if label_field in s]
                counter = Counter(str(l) for l in labels if l is not None)
                if counter:
                    sorted_cats = counter.most_common()
                    ratio = sorted_cats[0][1] / sorted_cats[-1][1] if sorted_cats[-1][1] > 0 else float("inf")
                    lines.append("### ç±»åˆ«åˆ†å¸ƒ")
                    lines.append(f"- ç±»åˆ«æ•°: {len(counter)}, ä¸å‡è¡¡æ¯”: {ratio:.1f}:1")
                    lines.append("| ç±»åˆ« | æ•°é‡ | å æ¯” |")
                    lines.append("|------|------|------|")
                    total = sum(counter.values())
                    for cat, cnt in sorted_cats[:30]:
                        lines.append(f"| {cat} | {cnt} | {cnt / total * 100:.1f}% |")
                    lines.append("")

            if "length" in dimensions and text_field:
                lengths = [len(s.get(text_field, "")) for s in samples if isinstance(s.get(text_field), str)]
                if lengths:
                    avg = sum(lengths) / len(lengths)
                    variance = sum((x - avg) ** 2 for x in lengths) / len(lengths)
                    std = math.sqrt(variance)
                    lines.append("### æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ")
                    lines.append(f"- å¹³å‡: {avg:.0f}, æ ‡å‡†å·®: {std:.0f}, èŒƒå›´: [{min(lengths)}, {max(lengths)}]")
                    lines.append("")

            if "language" in dimensions and text_field:
                lang_counter: Counter = Counter()
                for s in samples:
                    text = s.get(text_field, "")
                    if not isinstance(text, str) or not text:
                        continue
                    cjk = sum(1 for ch in text[:500] if "\u4e00" <= ch <= "\u9fff")
                    latin = sum(1 for ch in text[:500] if "\u0041" <= ch <= "\u007a")
                    total_c = cjk + latin or 1
                    if cjk / total_c > 0.3:
                        lang_counter["zh"] += 1
                    elif latin / total_c > 0.3:
                        lang_counter["en"] += 1
                    else:
                        lang_counter["other"] += 1
                if lang_counter:
                    lines.append("### è¯­è¨€åˆ†å¸ƒ")
                    total_l = sum(lang_counter.values())
                    for lang, cnt in lang_counter.most_common():
                        lines.append(f"- {lang}: {cnt} ({cnt / total_l * 100:.1f}%)")
                    lines.append("")

            return [TextContent(type="text", text="\n".join(lines))]

        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]

    return server


async def serve():
    """å¯åŠ¨ MCP æœåŠ¡å™¨."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream)


def main():
    """ä¸»å…¥å£."""
    import asyncio

    asyncio.run(serve())


if __name__ == "__main__":
    main()
