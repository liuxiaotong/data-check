"""DataCheck MCP Server - Model Context Protocol æœåŠ¡."""

from pathlib import Path
from typing import Any, Dict, List

try:
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import Tool, TextContent

    HAS_MCP = True
except ImportError:
    HAS_MCP = False

from datacheck.checker import DataChecker
from datacheck.fixer import DataFixer
from datacheck.report import QualityReport
from datacheck.rules import RuleSet, get_sft_ruleset, get_preference_ruleset


def create_server() -> "Server":
    """åˆ›å»º MCP æœåŠ¡å™¨å®ä¾‹."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = Server("datacheck")

    @server.list_tools()
    async def list_tools() -> List[Tool]:
        """åˆ—å‡ºå¯ç”¨çš„å·¥å…·."""
        return [
            Tool(
                name="check_data_quality",
                description="æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„è´¨é‡ (æ”¯æŒ JSON/JSONL/CSV)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "schema_path": {
                            "type": "string",
                            "description": "Schema æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                        "ruleset": {
                            "type": "string",
                            "enum": ["default", "sft", "preference"],
                            "description": "è§„åˆ™é›†ï¼ˆé»˜è®¤: defaultï¼‰",
                        },
                        "sample_count": {
                            "type": "integer",
                            "description": "éšæœºæŠ½æ ·æ•°é‡ï¼ˆå¯é€‰ï¼‰",
                        },
                        "sample_rate": {
                            "type": "number",
                            "description": "éšæœºæŠ½æ ·æ¯”ä¾‹ 0-1ï¼ˆå¯é€‰ï¼‰",
                        },
                    },
                    "required": ["data_path"],
                },
            ),
            Tool(
                name="validate_from_datarecipe",
                description="ä½¿ç”¨ DataRecipe åˆ†æç»“æœéªŒè¯æ•°æ®",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "analysis_dir": {
                            "type": "string",
                            "description": "DataRecipe åˆ†æè¾“å‡ºç›®å½•",
                        },
                        "data_path": {
                            "type": "string",
                            "description": "è¦éªŒè¯çš„æ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤éªŒè¯åˆæˆæ•°æ®ï¼‰",
                        },
                    },
                    "required": ["analysis_dir"],
                },
            ),
            Tool(
                name="compare_distributions",
                description="å¯¹æ¯”å¤šä¸ªæ•°æ®æ–‡ä»¶çš„åˆ†å¸ƒ (æ”¯æŒ JSON/JSONL/CSV)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "file_paths": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "è¦å¯¹æ¯”çš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (JSON/JSONL/CSV)",
                        },
                    },
                    "required": ["file_paths"],
                },
            ),
            Tool(
                name="list_quality_rules",
                description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è´¨é‡æ£€æŸ¥è§„åˆ™",
                inputSchema={
                    "type": "object",
                    "properties": {},
                },
            ),
            Tool(
                name="infer_schema",
                description="ä»æ•°æ®æ–‡ä»¶æ¨æ–­ Schema (å­—æ®µç±»å‹ã€çº¦æŸã€å¿…å¡«é¡¹)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "output_path": {
                            "type": "string",
                            "description": "Schema è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                    },
                    "required": ["data_path"],
                },
            ),
            Tool(
                name="fix_data",
                description="ä¿®å¤æ•°æ®æ–‡ä»¶å¸¸è§è´¨é‡é—®é¢˜ (å»é‡ã€å»ç©ºç™½ã€PII è„±æ•)",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ®æ–‡ä»¶è·¯å¾„ (JSON/JSONL/CSV)",
                        },
                        "output_path": {
                            "type": "string",
                            "description": "ä¿®å¤åæ–‡ä»¶è¾“å‡ºè·¯å¾„ (JSONL)",
                        },
                        "strip_pii": {
                            "type": "boolean",
                            "description": "æ˜¯å¦è„±æ• PII ä¿¡æ¯ï¼ˆé»˜è®¤: falseï¼‰",
                        },
                    },
                    "required": ["data_path", "output_path"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
        """è°ƒç”¨å·¥å…·."""

        if name == "check_data_quality":
            ruleset_name = arguments.get("ruleset", "default")
            if ruleset_name == "sft":
                rules = get_sft_ruleset()
            elif ruleset_name == "preference":
                rules = get_preference_ruleset()
            else:
                rules = RuleSet()

            checker = DataChecker(rules)
            result = checker.check_file(
                arguments["data_path"],
                arguments.get("schema_path"),
                sample_count=arguments.get("sample_count"),
                sample_rate=arguments.get("sample_rate"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"æ£€æŸ¥å¤±è´¥: {result.error}")]

            # Generate summary
            score = result.pass_rate * 100
            if score >= 90:
                grade = "ğŸŸ¢ ä¼˜ç§€"
            elif score >= 70:
                grade = "ğŸŸ¡ è‰¯å¥½"
            elif score >= 50:
                grade = "ğŸŸ  ä¸€èˆ¬"
            else:
                grade = "ğŸ”´ éœ€æ”¹è¿›"

            summary = f"""## æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ

### æ¦‚è¦
- æ€»æ ·æœ¬: {result.total_samples}
- é€šè¿‡: {result.passed_samples}
- å¤±è´¥: {result.failed_samples}
- **é€šè¿‡ç‡: {result.pass_rate:.1%}**
- **è¯„çº§: {grade}**

### é—®é¢˜ç»Ÿè®¡
- ğŸ”´ é”™è¯¯: {result.error_count}
- ğŸŸ¡ è­¦å‘Š: {result.warning_count}
- ğŸ”µ æç¤º: {result.info_count}
"""

            if result.duplicates:
                summary += f"\n### é‡å¤æ£€æµ‹\nå‘ç° {len(result.duplicates)} ç»„é‡å¤æ•°æ®\n"

            if result.failed_sample_ids:
                summary += f"\n### å¤±è´¥æ ·æœ¬\n{', '.join(result.failed_sample_ids[:10])}"
                if len(result.failed_sample_ids) > 10:
                    summary += f" (è¿˜æœ‰ {len(result.failed_sample_ids) - 10} ä¸ª...)"

            return [TextContent(type="text", text=summary)]

        elif name == "validate_from_datarecipe":
            checker = DataChecker()
            result = checker.check_from_datarecipe(
                arguments["analysis_dir"],
                arguments.get("data_path"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"éªŒè¯å¤±è´¥: {result.error}")]

            report = QualityReport(result, title="æ•°æ®éªŒè¯æŠ¥å‘Š")

            # Save report
            output_dir = Path(arguments["analysis_dir"]) / "12_è´¨æ£€æŠ¥å‘Š"
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / "quality_report.md"
            report.save(str(output_path), "markdown")

            # Return summary
            score = result.pass_rate * 100
            grade = (
                "ğŸŸ¢ ä¼˜ç§€"
                if score >= 90
                else "ğŸŸ¡ è‰¯å¥½"
                if score >= 70
                else "ğŸŸ  ä¸€èˆ¬"
                if score >= 50
                else "ğŸ”´ éœ€æ”¹è¿›"
            )

            return [
                TextContent(
                    type="text",
                    text=f"""## æ•°æ®éªŒè¯å®Œæˆ

### ç»“æœ
- é€šè¿‡ç‡: **{result.pass_rate:.1%}**
- è¯„çº§: **{grade}**
- æ€»æ ·æœ¬: {result.total_samples}
- é”™è¯¯: {result.error_count}, è­¦å‘Š: {result.warning_count}

### æŠ¥å‘Š
å·²ä¿å­˜åˆ°: {output_path}

{"### é‡å¤æ•°æ®" + chr(10) + f"å‘ç° {len(result.duplicates)} ç»„é‡å¤" if result.duplicates else ""}
""",
                )
            ]

        elif name == "compare_distributions":
            file_paths = arguments["file_paths"]

            if len(file_paths) < 2:
                return [TextContent(type="text", text="é”™è¯¯: è‡³å°‘éœ€è¦ 2 ä¸ªæ–‡ä»¶")]

            distributions = []
            for file_path in file_paths:
                checker = DataChecker()
                samples, _ = checker._load_data(Path(file_path))
                result = checker.check(samples, {})

                distributions.append(
                    {
                        "file": Path(file_path).name,
                        "count": len(samples),
                        "dist": result.distribution,
                    }
                )

            # Build comparison
            lines = ["## æ•°æ®åˆ†å¸ƒå¯¹æ¯”", ""]
            lines.append("| æ–‡ä»¶ | æ ·æœ¬æ•° |")
            lines.append("|------|--------|")
            for d in distributions:
                lines.append(f"| {d['file']} | {d['count']} |")

            lines.extend(["", "### å­—æ®µç»Ÿè®¡", ""])

            all_fields = set()
            for d in distributions:
                all_fields.update(d["dist"].get("fields", {}).keys())

            for field in sorted(all_fields):
                lines.append(f"**{field}**:")
                for d in distributions:
                    field_data = d["dist"].get("fields", {}).get(field, {})
                    if "length_stats" in field_data:
                        stats = field_data["length_stats"]
                        lines.append(f"- {d['file']}: é•¿åº¦ {stats['avg']:.0f} (avg)")

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "list_quality_rules":
            ruleset = RuleSet()
            lines = ["## å¯ç”¨è´¨é‡æ£€æŸ¥è§„åˆ™", ""]

            for rule in ruleset.rules.values():
                severity_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(
                    rule.severity.value, "âšª"
                )
                status = "âœ“" if rule.enabled else "âœ—"
                lines.append(f"- {status} **{rule.name}** {severity_icon}")
                lines.append(f"  - ID: `{rule.id}`")
                lines.append(f"  - {rule.description}")
                lines.append("")

            lines.extend(
                [
                    "## é¢„è®¾è§„åˆ™é›†",
                    "- `default`: é€šç”¨è§„åˆ™",
                    "- `sft`: SFT æ•°æ®è§„åˆ™",
                    "- `preference`: åå¥½æ•°æ®è§„åˆ™",
                ]
            )

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "infer_schema":
            checker = DataChecker()
            schema = checker.infer_schema_file(
                arguments["data_path"],
                arguments.get("output_path"),
            )

            fields = schema.get("fields", {})
            field_count = len(fields)
            required_count = sum(1 for f in fields.values() if f.get("required"))

            lines = [
                "## Schema æ¨æ–­ç»“æœ",
                "",
                f"- æ ·æœ¬æ•°: {schema.get('sample_count', 0)}",
                f"- å­—æ®µæ•°: {field_count}",
                f"- å¿…å¡«å­—æ®µ: {required_count}",
                "",
                "### å­—æ®µè¯¦æƒ…",
                "",
                "| å­—æ®µ | ç±»å‹ | å¿…å¡« | çº¦æŸ |",
                "|------|------|------|------|",
            ]

            for fname, fdef in fields.items():
                ftype = fdef.get("type", "-")
                req = "æ˜¯" if fdef.get("required") else "å¦"
                constraints = []
                if "min_length" in fdef:
                    constraints.append(f"é•¿åº¦ {fdef['min_length']}-{fdef['max_length']}")
                if "enum" in fdef:
                    constraints.append(f"æšä¸¾ {fdef['enum']}")
                if "min_value" in fdef:
                    constraints.append(f"å€¼ {fdef['min_value']}-{fdef['max_value']}")
                lines.append(f"| {fname} | {ftype} | {req} | {', '.join(constraints) or '-'} |")

            if arguments.get("output_path"):
                lines.extend(["", f"Schema å·²ä¿å­˜: {arguments['output_path']}"])

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "fix_data":
            fixer = DataFixer()
            result = fixer.fix_file(
                arguments["data_path"],
                arguments["output_path"],
                strip_pii=arguments.get("strip_pii", False),
            )

            lines = [
                "## æ•°æ®ä¿®å¤ç»“æœ",
                "",
                f"- è¾“å…¥æ ·æœ¬: {result.total_input}",
                f"- è¾“å‡ºæ ·æœ¬: {result.total_output}",
            ]
            if result.duplicates_removed:
                lines.append(f"- å»é™¤é‡å¤: {result.duplicates_removed}")
            if result.trimmed_count:
                lines.append(f"- ä¿®å‰ªç©ºç™½: {result.trimmed_count} ä¸ªå­—æ®µ")
            if result.empty_removed:
                lines.append(f"- ç§»é™¤ç©ºæ ·æœ¬: {result.empty_removed}")
            if result.pii_redacted_count:
                lines.append(f"- PII è„±æ•: {result.pii_redacted_count} ä¸ªå­—æ®µ")
            lines.extend(["", f"è¾“å‡ºæ–‡ä»¶: {arguments['output_path']}"])

            return [TextContent(type="text", text="\n".join(lines))]

        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]

    return server


async def serve():
    """å¯åŠ¨ MCP æœåŠ¡å™¨."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream)


def main():
    """ä¸»å…¥å£."""
    import asyncio

    asyncio.run(serve())


if __name__ == "__main__":
    main()
