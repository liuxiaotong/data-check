"""DataCheck MCP Server - Model Context Protocol æœåŠ¡."""

import json
from pathlib import Path
from typing import Any, Dict, List

try:
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import Tool, TextContent
    HAS_MCP = True
except ImportError:
    HAS_MCP = False

from datacheck.checker import DataChecker
from datacheck.report import QualityReport
from datacheck.rules import RuleSet, get_sft_ruleset, get_preference_ruleset


def create_server() -> "Server":
    """åˆ›å»º MCP æœåŠ¡å™¨å®ä¾‹."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = Server("datacheck")

    @server.list_tools()
    async def list_tools() -> List[Tool]:
        """åˆ—å‡ºå¯ç”¨çš„å·¥å…·."""
        return [
            Tool(
                name="check_data_quality",
                description="æ£€æŸ¥æ•°æ®æ–‡ä»¶çš„è´¨é‡",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "data_path": {
                            "type": "string",
                            "description": "æ•°æ® JSON æ–‡ä»¶è·¯å¾„",
                        },
                        "schema_path": {
                            "type": "string",
                            "description": "Schema æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        },
                        "ruleset": {
                            "type": "string",
                            "enum": ["default", "sft", "preference"],
                            "description": "è§„åˆ™é›†ï¼ˆé»˜è®¤: defaultï¼‰",
                        },
                    },
                    "required": ["data_path"],
                },
            ),
            Tool(
                name="validate_from_datarecipe",
                description="ä½¿ç”¨ DataRecipe åˆ†æç»“æœéªŒè¯æ•°æ®",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "analysis_dir": {
                            "type": "string",
                            "description": "DataRecipe åˆ†æè¾“å‡ºç›®å½•",
                        },
                        "data_path": {
                            "type": "string",
                            "description": "è¦éªŒè¯çš„æ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤éªŒè¯åˆæˆæ•°æ®ï¼‰",
                        },
                    },
                    "required": ["analysis_dir"],
                },
            ),
            Tool(
                name="compare_distributions",
                description="å¯¹æ¯”å¤šä¸ªæ•°æ®æ–‡ä»¶çš„åˆ†å¸ƒ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "file_paths": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "è¦å¯¹æ¯”çš„æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨",
                        },
                    },
                    "required": ["file_paths"],
                },
            ),
            Tool(
                name="list_quality_rules",
                description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è´¨é‡æ£€æŸ¥è§„åˆ™",
                inputSchema={
                    "type": "object",
                    "properties": {},
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
        """è°ƒç”¨å·¥å…·."""

        if name == "check_data_quality":
            ruleset_name = arguments.get("ruleset", "default")
            if ruleset_name == "sft":
                rules = get_sft_ruleset()
            elif ruleset_name == "preference":
                rules = get_preference_ruleset()
            else:
                rules = RuleSet()

            checker = DataChecker(rules)
            result = checker.check_file(
                arguments["data_path"],
                arguments.get("schema_path"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"æ£€æŸ¥å¤±è´¥: {result.error}")]

            # Generate summary
            score = result.pass_rate * 100
            if score >= 90:
                grade = "ğŸŸ¢ ä¼˜ç§€"
            elif score >= 70:
                grade = "ğŸŸ¡ è‰¯å¥½"
            elif score >= 50:
                grade = "ğŸŸ  ä¸€èˆ¬"
            else:
                grade = "ğŸ”´ éœ€æ”¹è¿›"

            summary = f"""## æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ

### æ¦‚è¦
- æ€»æ ·æœ¬: {result.total_samples}
- é€šè¿‡: {result.passed_samples}
- å¤±è´¥: {result.failed_samples}
- **é€šè¿‡ç‡: {result.pass_rate:.1%}**
- **è¯„çº§: {grade}**

### é—®é¢˜ç»Ÿè®¡
- ğŸ”´ é”™è¯¯: {result.error_count}
- ğŸŸ¡ è­¦å‘Š: {result.warning_count}
- ğŸ”µ æç¤º: {result.info_count}
"""

            if result.duplicates:
                summary += f"\n### é‡å¤æ£€æµ‹\nå‘ç° {len(result.duplicates)} ç»„é‡å¤æ•°æ®\n"

            if result.failed_sample_ids:
                summary += f"\n### å¤±è´¥æ ·æœ¬\n{', '.join(result.failed_sample_ids[:10])}"
                if len(result.failed_sample_ids) > 10:
                    summary += f" (è¿˜æœ‰ {len(result.failed_sample_ids) - 10} ä¸ª...)"

            return [TextContent(type="text", text=summary)]

        elif name == "validate_from_datarecipe":
            checker = DataChecker()
            result = checker.check_from_datarecipe(
                arguments["analysis_dir"],
                arguments.get("data_path"),
            )

            if not result.success:
                return [TextContent(type="text", text=f"éªŒè¯å¤±è´¥: {result.error}")]

            report = QualityReport(result, title="æ•°æ®éªŒè¯æŠ¥å‘Š")

            # Save report
            output_dir = Path(arguments["analysis_dir"]) / "12_è´¨æ£€æŠ¥å‘Š"
            output_dir.mkdir(exist_ok=True)
            output_path = output_dir / "quality_report.md"
            report.save(str(output_path), "markdown")

            # Return summary
            score = result.pass_rate * 100
            grade = "ğŸŸ¢ ä¼˜ç§€" if score >= 90 else "ğŸŸ¡ è‰¯å¥½" if score >= 70 else "ğŸŸ  ä¸€èˆ¬" if score >= 50 else "ğŸ”´ éœ€æ”¹è¿›"

            return [TextContent(
                type="text",
                text=f"""## æ•°æ®éªŒè¯å®Œæˆ

### ç»“æœ
- é€šè¿‡ç‡: **{result.pass_rate:.1%}**
- è¯„çº§: **{grade}**
- æ€»æ ·æœ¬: {result.total_samples}
- é”™è¯¯: {result.error_count}, è­¦å‘Š: {result.warning_count}

### æŠ¥å‘Š
å·²ä¿å­˜åˆ°: {output_path}

{"### é‡å¤æ•°æ®" + chr(10) + f"å‘ç° {len(result.duplicates)} ç»„é‡å¤" if result.duplicates else ""}
"""
            )]

        elif name == "compare_distributions":
            file_paths = arguments["file_paths"]

            if len(file_paths) < 2:
                return [TextContent(type="text", text="é”™è¯¯: è‡³å°‘éœ€è¦ 2 ä¸ªæ–‡ä»¶")]

            distributions = []
            for file_path in file_paths:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                samples = data.get("samples", data.get("responses", data if isinstance(data, list) else []))
                checker = DataChecker()
                result = checker.check(samples, {})

                distributions.append({
                    "file": Path(file_path).name,
                    "count": len(samples),
                    "dist": result.distribution,
                })

            # Build comparison
            lines = ["## æ•°æ®åˆ†å¸ƒå¯¹æ¯”", ""]
            lines.append("| æ–‡ä»¶ | æ ·æœ¬æ•° |")
            lines.append("|------|--------|")
            for d in distributions:
                lines.append(f"| {d['file']} | {d['count']} |")

            lines.extend(["", "### å­—æ®µç»Ÿè®¡", ""])

            all_fields = set()
            for d in distributions:
                all_fields.update(d["dist"].get("fields", {}).keys())

            for field in sorted(all_fields):
                lines.append(f"**{field}**:")
                for d in distributions:
                    field_data = d["dist"].get("fields", {}).get(field, {})
                    if "length_stats" in field_data:
                        stats = field_data["length_stats"]
                        lines.append(f"- {d['file']}: é•¿åº¦ {stats['avg']:.0f} (avg)")

            return [TextContent(type="text", text="\n".join(lines))]

        elif name == "list_quality_rules":
            ruleset = RuleSet()
            lines = ["## å¯ç”¨è´¨é‡æ£€æŸ¥è§„åˆ™", ""]

            for rule in ruleset.rules.values():
                severity_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(rule.severity.value, "âšª")
                status = "âœ“" if rule.enabled else "âœ—"
                lines.append(f"- {status} **{rule.name}** {severity_icon}")
                lines.append(f"  - ID: `{rule.id}`")
                lines.append(f"  - {rule.description}")
                lines.append("")

            lines.extend([
                "## é¢„è®¾è§„åˆ™é›†",
                "- `default`: é€šç”¨è§„åˆ™",
                "- `sft`: SFT æ•°æ®è§„åˆ™",
                "- `preference`: åå¥½æ•°æ®è§„åˆ™",
            ])

            return [TextContent(type="text", text="\n".join(lines))]

        else:
            return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]

    return server


async def serve():
    """å¯åŠ¨ MCP æœåŠ¡å™¨."""
    if not HAS_MCP:
        raise ImportError("MCP æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datacheck[mcp]")

    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream)


def main():
    """ä¸»å…¥å£."""
    import asyncio
    asyncio.run(serve())


if __name__ == "__main__":
    main()
